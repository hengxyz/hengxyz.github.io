---
title: "<font color=blue><u>SEDUCTION (Project Idex)</u></font>"
collection: expressions
excerpt: "Social affects discrimination using combined acoustic and visual information in the multicultural environment (Japanese/French)."
redirect_to: 
---

2014-2015 : This project aims to ananlyse the variation of facial expressions in the multicultural environment using combined acoustic and visual information with the machine learning methods:

- Collabration labotories: [LaBRI](https://www.labri.fr/), [University of Kyoto](https://www.kyoto-u.ac.jp/en)
- with [Aur√©lie Bugeau](https://www.labri.fr/perso/bugeau/), [Jean-Luc ROUAS](https://scholar.google.fr/citations?user=aWKht5IAAAAJ&hl=fr), [Takaaki SHOCHI](https://erssab.u-bordeaux-montaigne.fr/membres/titulaires/9-shochi-takaaki), [Toyoaki Nishida](https://sites.google.com/view/toyoakinishida/home)

**Reasearch methods**: The challenges of face anti-spoofing is to distinguish the real presentation from the presentation attacks with fine difference in terms of image texture. The methods based on **long/short-term attention for fine-grained classification, multimodal learning,  multitask learning** are proposed for FAS in this project. 

- [VitTransPAD: Video Transformer using Convolution and Self-attention for Face Presentation Attack Detection (ICIP2022_submission):](https://arxiv.org/pdf/2203.01562.pdf)

*Key words: Video-based transformer, multi-scale multi-head self-attention, face presentation attack detection*

<font size=3>Many works based on Convolution Neural Networks (CNNs) for face PAD formulate the problem as an image-level binary classification task without considering the context. Alternatively, Vision Transformers (ViT) using self-attention to attend the context of an image become the mainstreams in face PAD. Inspired by ViT, we design a Video-based Transformer for face PAD with short/long-range spatio-temporal attention which can not only focus on local details but also the context of a video. The proposed Multi-scale Multi-Head Self-Attention enables the model to learn a fine-grained representation to perform pixel level discrimination required by face PAD. We also introduce convolutions to our ViTransPAD to integrate desirable proprieties of CNNs which can gain a good computation-accuracy balance. To the best of our knowledge, this is the first approach using video-based transformer for face PAD which can serve as a new backbone for further study.</font>
